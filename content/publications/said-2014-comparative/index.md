---
title: 'Comparative recommender system evaluation: Benchmarking recommendation frameworks'
type: publication 
profile: false
# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- alan
- A. Bellog√≠n

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2014-01-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2023-12-23T22:33:30.702017Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- paper-conference

# Publication name and optional abbreviated publication name.
publication: '*RecSys 2014 - Proceedings of the 8th ACM Conference on Recommender
  Systems*'
publication_short: ''

doi: 10.1145/2645710.2645746

abstract: 'Recommender systems research is often based on comparisons of predictive
  accuracy: the better the evaluation scores, the better the recommender. However,
  it is difficult to compare results from different recommender systems due to the
  many options in design and implementation of an evaluation strategy. Additionally,
  algorithmic implementations can diverge from the standard formulation due to manual
  tuning and modifications that work better in some situations. In this work we compare
  common recommendation algorithms as implemented in three popular recommendation
  frameworks. To provide a fair comparison, we have complete control of the evaluation
  dimensions being benchmarked: dataset, data splitting, evaluation strategies, and
  metrics. We also include results using the internal evaluation mechanisms of these
  frameworks. Our analysis points to large differences in recommendation accuracy
  across frameworks and strategies, i.e. the same baselines may perform orders of
  magnitude better or worse across frameworks. Our results show the necessity of clear
  guidelines when reporting evaluation of recommender systems to ensure reproducibility
  and comparison of results.'

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
---


