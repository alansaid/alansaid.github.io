---
type: publication
title: 'Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives'
profile: false
# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- Christine Bauer
- Eva Zangerle
- alan

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2023-10-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2023-12-23T22:33:31.370361Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- article-journal

# Publication name and optional abbreviated publication name.
publication: '*ACM Transactions on Recommender Systems*'
publication_short: ''



abstract: Recommender systems research and practice are fast-developing topics with
  growing adoption in a wide variety of information access scenarios. In this paper,
  we present an overview of research specifically focused on the emphevaluation of
  recommender systems. We perform a systematic literature review, in which we analyze
  57 papers spanning six years (2017–2022). Focusing on the processes surrounding
  evaluation, we dial in on the methods applied, the datasets utilized, and the metrics
  used. Our study shows that the predominant experiment type in research on the evaluation
  of recommender systems is offline experimentation and that online evaluations are
  primarily used in combination with other experimentation methods, e.g., an offline
  experiment. Furthermore, we find that only a few datasets (MovieLens, Amazon review
  dataset) are widely used, while many datasets are used in only a few papers each.
  We observe a similar scenario when analyzing the employed performance metrics—a
  few metrics are widely used (precision, nDCG, and Recall), while many others are
  used in only a few papers. Overall, our review indicates that beyond-accuracy qualities
  are rarely assessed. Our analysis shows that the research community working on evaluation
  has focused on the development of evaluation in a rather narrow scope, with the
  majority of experiments focusing on a few metrics, datasets, and methods.

# Summary. An optional shortened abstract.
summary: ''

tags:
- recommender systems
- evaluation
- survey
- systematic literature review

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false
#doi: 10.1145/3629170


# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
links:
- icon: acm
  icon_pack: ai
  name: ACM
  url: https://dl.acm.org/doi/10.1145/3629170?cid=81413593442
- icon: doi
  icon_pack: ai
  name: DOI
  url: https://doi.org/10.1145/3629170
---

