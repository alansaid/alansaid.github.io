---
title: Evaluating the Accuracy and Utility of Recommender Systems
type: publication 
profile: false
# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- alan

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2013-04-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2023-12-23T22:33:31.118260Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- thesis

# Publication name and optional abbreviated publication name.
publication: ''
publication_short: ''

doi: ''

abstract: Recommender systems have become a ubiquitous feature on the World Wide Web.
  Today, most websites use some form of recommendation to heighten their users’ experience.
  Over the last decade, vast advancements in recommendation have been done, this has
  however not been matched in the processes involved in evaluating these systems.
  The evaluation methods and metrics currently used for this have originated in other
  related fields, e.g. information retrieval, statistics, etc. For most cases, these
  evaluation processes are able to show how well a recommender system performs – to
  some point. However, after a certain threshold, it is not often clear whether a
  lower error, or higher accuracy metric accounts for an actual quality improvement.  This
  dissertation focuses on the research question how can we further estimate whether
  a measured accuracy level actually corresponds to a quality improvement from the
  user’s perspective, or whether the measured improvement is lost on the end user.
  We introduce some of the concepts related to recommendation quality and user perception,
  and continue on to show that currently widely-used evaluation metrics do not capture
  the quality of recommendation when the algorithm is specifically tuned to offer
  recommendation of a higher diversity. Following this we present a formalization
  of the upper limit of recommendation quality, a magic barrier of recommendation,
  and evaluate it in a real-world movie recommendation setting.  The work presented
  in this dissertation concludes that current recommendation quality has outgrown
  the methods and metrics used for the evaluation of these systems. Instead, we show
  how qualitative approaches can be used, with minimal user interference, to correctly
  estimate the actual quality of recommendation systems.

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
links:
- name: URL
  url: 
    https://api-depositonce.tu-berlin.de/server/api/core/bitstreams/b490302e-8b6a-4300-946f-b5763c2b47d7/content
---


