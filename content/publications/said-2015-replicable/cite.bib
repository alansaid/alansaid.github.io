@inproceedings{said_replicable_2015,
 abstract = {Recommender systems research is by and large based on comparisons of recommendation algorithms' predictive accuracies: the better the evaluation metrics (higher accuracy scores or lower predictive errors), the better the recommendation algorithm. Comparing the evaluation results of two recommendation approaches is however a difficult process as there are very many factors to be considered in the implementation of an algorithm, its evaluation, and how datasets are processed and prepared. This tutorial shows how to present evaluation results in a clear and concise manner, while ensuring that the results are comparable, replicable and unbiased. These insights are not limited to recommender systems research alone, but are also valid for experiments with other types of personalized interactions and contextual information access.},
 author = {Said, A. and Bellog√≠n, A.},
 booktitle = {RecSys 2015 - Proceedings of the 9th ACM Conference on Recommender Systems},
 copyright = {All rights reserved},
 doi = {10.1145/2792838.2792841},
 isbn = {978-1-4503-3692-5},
 keywords = {Evaluation, Reproducibility, Experimental design, Experimental methodology, Replicability},
 title = {Replicable evaluation of recommender systems},
 year = {2015}
}
