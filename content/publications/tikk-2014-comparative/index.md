---
title: Comparative evaluation of recommender systems for digital media
type: publication 
profile: false
# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- D. Tikk
- R. Turrin
- M. Larson
- D. Zibriczky
- D. Malagoli
- alan
- A. Lommatzsch
- V. Gál
- S. Székely

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2014-01-01'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2023-12-23T22:33:31.184478Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- paper-conference

# Publication name and optional abbreviated publication name.
publication: 'IBC2014 Conference'
publication_short: 'IBC'

doi: 10.1049/ib.2014.0015

abstract: TV operators and content providers use recommender systems to connect consumers
  directly with content that fits their needs, their different devices, and the context
  in which the content is being consumed. Choosing the right recommender algorithms
  is critical, and becomes more difficult as content offerings continue to radically
  expand. Because different algorithms respond differently depending on the use-case,
  including the content and the consumer base, theoretical estimates of performance
  are not sufficient. Rather, evaluation must be carried out in a realistic environment.
  The Reference Framework described here is an evaluation platform that enables TV
  operators to compare impartially not just the qualitative aspects of recommendation
  algorithms, but also non-functional requirements of complete recommendation solutions.
  The Reference Framework is being created by the CrowdRec project which includes
  the most innovative recommendation system vendors and university researchers in
  the specific fields of recommendation systems and their evaluation. It provides
  batch-based evaluation modes and looks forward to supporting stream-based modes
  in the future. It is also able to encapsulate open source recommender and evaluation
  frameworks, making it suitable for a wide scope of evaluation needs.

# Summary. An optional shortened abstract.
summary: ''

tags: []

# Display this page in a list of Featured pages?
featured: false

# Links
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
links:
- name: URL
  url: https://digital-library.theiet.org/content/conferences/10.1049/ib.2014.0015
---


