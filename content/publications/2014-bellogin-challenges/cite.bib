@inproceedings{bellogin_challenges_2014,
 abstract = {The TREC 2013 Contextual Suggestion Track allowed participants to submit personalised rankings using documents either from the OpenWeb or from an archived, static Web collection, the ClueWeb12 dataset. We argue that this setting poses problems in how the performance of the participants should be compared. We analyse biases found in the process, both objective and subjective, and discuss these issues in the general framework of evaluating personalised Information Retrieval using dynamic against static datasets.},
 address = {Cham},
 author = {Bellog√≠n, Alejandro and Samar, Thaer and de Vries, Arjen P. and Said, Alan},
 booktitle = {Advances in Information Retrieval},
 copyright = {All rights reserved},
 doi = {10.1007/978-3-319-06028-6_37},
 editor = {de Rijke, Maarten and Kenter, Tom and de Vries, Arjen P. and Zhai, ChengXiang and de Jong, Franciska and Radinsky, Kira and Hofmann, Katja},
 isbn = {978-3-319-06028-6},
 keywords = {Information Retrieval, Evaluation Judgement, Information Retrieval System, Mean Reciprocal Rank, Relevance Judgement},
 language = {en},
 pages = {430--436},
 publisher = {Springer International Publishing},
 series = {Lecture Notes in Computer Science},
 shorttitle = {Challenges on Combining Open Web and Dataset Evaluation Results},
 title = {Challenges on Combining Open Web and Dataset Evaluation Results: The Case of the Contextual Suggestion Track},
 year = {2014}
}
